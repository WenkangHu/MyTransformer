[2021-06-23 12:18:30,428][utils.py][line:129][INFO] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2021-06-23 12:18:30,430][utils.py][line:141][INFO] NumExpr defaulting to 8 threads.
[2021-06-23 12:18:31,221][preprocess.py][line:89][INFO] load src_vocab from ./checkpoints/vocabs/en-src_vocab.pkl
[2021-06-23 12:18:31,412][preprocess.py][line:97][INFO] load tgt_vocab from ./checkpoints/vocabs/zh-tgt_vocab.pkl
[2021-06-23 12:18:31,413][preprocess.py][line:109][INFO] train and val data cache alreadly existed
[2021-06-23 12:18:31,415][preprocess.py][line:111][INFO] src vocab size:24454 tgt vocab size:6870 train examples:282305 val examples:31368
[2021-06-23 12:18:45,469][train.py][line:264][INFO] ==== Model Structure: =====
[2021-06-23 12:18:45,469][train.py][line:265][INFO] Transformer(
  (encoder): Encoder(
    (src_emb): Embedding(24454, 512, padding_idx=1)
    (pos_enc): PosEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (layers): ModuleList(
      (0): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (1): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (2): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (3): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (4): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (5): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
    )
  )
  (decoder): Decoder(
    (tgt_emb): Embedding(6870, 512, padding_idx=1)
    (pos_enc): PosEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (layers): ModuleList(
      (0): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (1): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (2): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (3): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (4): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (5): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
    )
  )
  (project): Linear(in_features=512, out_features=6870, bias=False)
)
[2021-06-23 12:18:45,483][train.py][line:267][INFO] ==== Training/Evaluation Parameters: =====
[2021-06-23 12:18:45,487][train.py][line:269][INFO] 	adam_eps=1e-09
[2021-06-23 12:18:45,489][train.py][line:269][INFO] 	batch_size=200
[2021-06-23 12:18:45,493][train.py][line:269][INFO] 	beta1=0.9
[2021-06-23 12:18:45,496][train.py][line:269][INFO] 	beta2=0.98
[2021-06-23 12:18:45,499][train.py][line:269][INFO] 	checkpoints=./checkpoints
[2021-06-23 12:18:45,502][train.py][line:269][INFO] 	cuda=True
[2021-06-23 12:18:45,505][train.py][line:269][INFO] 	d_ff=2048
[2021-06-23 12:18:45,506][train.py][line:269][INFO] 	d_k=64
[2021-06-23 12:18:45,507][train.py][line:269][INFO] 	d_model=512
[2021-06-23 12:18:45,507][train.py][line:269][INFO] 	d_v=64
[2021-06-23 12:18:45,508][train.py][line:269][INFO] 	data_dir=./data/news-commentary-v16.en-zh.tsv
[2021-06-23 12:18:45,510][train.py][line:269][INFO] 	device=cuda
[2021-06-23 12:18:45,511][train.py][line:269][INFO] 	drop=0.1
[2021-06-23 12:18:45,512][train.py][line:269][INFO] 	epochs=30
[2021-06-23 12:18:45,512][train.py][line:269][INFO] 	eval=True
[2021-06-23 12:18:45,513][train.py][line:269][INFO] 	labelsmooth=True
[2021-06-23 12:18:45,514][train.py][line:269][INFO] 	log=True
[2021-06-23 12:18:45,514][train.py][line:269][INFO] 	log_dir=./results/logs
[2021-06-23 12:18:45,515][train.py][line:269][INFO] 	log_step=200
[2021-06-23 12:18:45,516][train.py][line:269][INFO] 	min_freq=1
[2021-06-23 12:18:45,517][train.py][line:269][INFO] 	num_head=8
[2021-06-23 12:18:45,518][train.py][line:269][INFO] 	num_layer=6
[2021-06-23 12:18:45,518][train.py][line:269][INFO] 	overwrite_data_cache=False
[2021-06-23 12:18:45,519][train.py][line:269][INFO] 	overwrite_vocab=False
[2021-06-23 12:18:45,519][train.py][line:269][INFO] 	pad_idx=1
[2021-06-23 12:18:45,520][train.py][line:269][INFO] 	result_dir=./results
[2021-06-23 12:18:45,520][train.py][line:269][INFO] 	save_step=1000
[2021-06-23 12:18:45,521][train.py][line:269][INFO] 	scale_emb=True
[2021-06-23 12:18:45,521][train.py][line:269][INFO] 	seed=123
[2021-06-23 12:18:45,522][train.py][line:269][INFO] 	share_emb_weight=False
[2021-06-23 12:18:45,522][train.py][line:269][INFO] 	share_proj_weight=True
[2021-06-23 12:18:45,523][train.py][line:269][INFO] 	smooth_eps=0.1
[2021-06-23 12:18:45,523][train.py][line:269][INFO] 	src_lang=en
[2021-06-23 12:18:45,524][train.py][line:269][INFO] 	src_max_seq_len=100
[2021-06-23 12:18:45,524][train.py][line:269][INFO] 	src_tokenizer_dir=./models/bert-base-uncased
[2021-06-23 12:18:45,524][train.py][line:269][INFO] 	src_vocab_dir=./checkpoints/vocabs/en-src_vocab.pkl
[2021-06-23 12:18:45,524][train.py][line:269][INFO] 	src_vocab_size=24454
[2021-06-23 12:18:45,525][train.py][line:269][INFO] 	task_name=en_zh_translation
[2021-06-23 12:18:45,525][train.py][line:269][INFO] 	tgt_lang=zh
[2021-06-23 12:18:45,525][train.py][line:269][INFO] 	tgt_max_seq_len=100
[2021-06-23 12:18:45,525][train.py][line:269][INFO] 	tgt_tokenizer_dir=./models/bert-base-chinese
[2021-06-23 12:18:45,525][train.py][line:269][INFO] 	tgt_vocab_dir=./checkpoints/vocabs/zh-tgt_vocab.pkl
[2021-06-23 12:18:45,526][train.py][line:269][INFO] 	tgt_vocab_size=6870
[2021-06-23 12:18:45,526][train.py][line:269][INFO] 	train=True
[2021-06-23 12:18:45,526][train.py][line:269][INFO] 	train_data_cache=./data/train_data_cache.pkl
[2021-06-23 12:18:45,526][train.py][line:269][INFO] 	use_tb=True
[2021-06-23 12:18:45,526][train.py][line:269][INFO] 	val_data_cache=./data/val_data_cache.pkl
[2021-06-23 12:18:45,526][train.py][line:269][INFO] 	vocab_dir=./checkpoints/vocabs
[2021-06-23 12:18:45,527][train.py][line:269][INFO] 	warmup_proportion=0.1
[2021-06-23 12:18:45,527][train.py][line:270][INFO] ==== Parameters End =====

[2021-06-23 12:18:45,527][train.py][line:273][INFO] ==== Train Start=====
[2021-06-23 12:18:47,741][train.py][line:147][INFO] Total train steps: 42360
[2021-06-23 12:18:47,745][train.py][line:148][INFO] ################################
[2021-06-23 12:22:09,540][train.py][line:181][INFO] #####Evaluate: global step 200#####
[2021-06-23 12:22:09,541][train.py][line:183][INFO] eval_loss_per_word = 7.031626404714599
[2021-06-23 12:22:09,541][train.py][line:183][INFO] eval_ppl = 1131.869992683141
[2021-06-23 12:22:09,541][train.py][line:183][INFO] eval_acc = 0.04479924156563398
[2021-06-23 12:25:33,006][train.py][line:181][INFO] #####Evaluate: global step 400#####
[2021-06-23 12:25:33,012][train.py][line:183][INFO] eval_loss_per_word = 5.313408667801498
[2021-06-23 12:25:33,012][train.py][line:183][INFO] eval_ppl = 203.04114998840035
[2021-06-23 12:25:33,012][train.py][line:183][INFO] eval_acc = 0.2706716248751769
[2021-06-23 12:29:03,936][train.py][line:181][INFO] #####Evaluate: global step 600#####
[2021-06-23 12:29:03,943][train.py][line:183][INFO] eval_loss_per_word = 3.567352065896548
[2021-06-23 12:29:03,945][train.py][line:183][INFO] eval_ppl = 35.422671956829014
[2021-06-23 12:29:03,946][train.py][line:183][INFO] eval_acc = 0.4939211914403514
[2021-06-23 12:32:35,795][train.py][line:181][INFO] #####Evaluate: global step 800#####
[2021-06-23 12:32:35,800][train.py][line:183][INFO] eval_loss_per_word = 2.434856711943599
[2021-06-23 12:32:35,802][train.py][line:183][INFO] eval_ppl = 11.414183080077434
[2021-06-23 12:32:35,804][train.py][line:183][INFO] eval_acc = 0.7782552027494777
[2021-06-23 12:36:05,732][train.py][line:181][INFO] #####Evaluate: global step 1000#####
[2021-06-23 12:36:05,740][train.py][line:183][INFO] eval_loss_per_word = 1.7843393531145566
[2021-06-23 12:36:05,745][train.py][line:183][INFO] eval_ppl = 5.955644069907482
[2021-06-23 12:36:05,746][train.py][line:183][INFO] eval_acc = 0.9147404598391237
[2021-06-23 12:36:07,976][train.py][line:191][INFO] global step 1000: save model to ./checkpoints/1000.pth
[2021-06-23 12:39:21,463][train.py][line:181][INFO] #####Evaluate: global step 1200#####
[2021-06-23 12:39:21,464][train.py][line:183][INFO] eval_loss_per_word = 1.5121848159802098
[2021-06-23 12:39:21,464][train.py][line:183][INFO] eval_ppl = 4.536631679959942
[2021-06-23 12:39:21,465][train.py][line:183][INFO] eval_acc = 0.9636075561627387
[2021-06-23 12:42:50,881][train.py][line:181][INFO] #####Evaluate: global step 1400#####
[2021-06-23 12:42:50,883][train.py][line:183][INFO] eval_loss_per_word = 1.3723653846186061
[2021-06-23 12:42:50,884][train.py][line:183][INFO] eval_ppl = 3.944670331390774
[2021-06-23 12:42:50,884][train.py][line:183][INFO] eval_acc = 0.9874426426352837
[2021-06-23 12:46:23,362][train.py][line:181][INFO] #####Evaluate: global step 1600#####
[2021-06-23 12:46:23,363][train.py][line:183][INFO] eval_loss_per_word = 1.711904678631242
[2021-06-23 12:46:23,364][train.py][line:183][INFO] eval_ppl = 5.539502407758886
[2021-06-23 12:46:23,364][train.py][line:183][INFO] eval_acc = 0.9336469481899884
[2021-06-23 12:49:42,925][train.py][line:181][INFO] #####Evaluate: global step 1800#####
[2021-06-23 12:49:42,932][train.py][line:183][INFO] eval_loss_per_word = 1.3244516431444358
[2021-06-23 12:49:42,932][train.py][line:183][INFO] eval_ppl = 3.760122900400833
[2021-06-23 12:49:42,933][train.py][line:183][INFO] eval_acc = 0.9939219572262622
[2021-06-23 12:53:01,395][train.py][line:181][INFO] #####Evaluate: global step 2000#####
[2021-06-23 12:53:01,396][train.py][line:183][INFO] eval_loss_per_word = 1.31228433928207
[2021-06-23 12:53:01,396][train.py][line:183][INFO] eval_ppl = 3.7146495475632775
[2021-06-23 12:53:01,396][train.py][line:183][INFO] eval_acc = 0.9947872953054261
[2021-06-23 12:53:01,784][train.py][line:191][INFO] global step 2000: save model to ./checkpoints/2000.pth
[2021-06-23 12:56:10,517][train.py][line:181][INFO] #####Evaluate: global step 2200#####
[2021-06-23 12:56:10,518][train.py][line:183][INFO] eval_loss_per_word = 1.3171715154156534
[2021-06-23 12:56:10,518][train.py][line:183][INFO] eval_ppl = 3.732848127788255
[2021-06-23 12:56:10,519][train.py][line:183][INFO] eval_acc = 0.9954428080450405
[2021-06-23 12:59:19,041][train.py][line:181][INFO] #####Evaluate: global step 2400#####
[2021-06-23 12:59:19,042][train.py][line:183][INFO] eval_loss_per_word = 1.347365302954718
[2021-06-23 12:59:19,043][train.py][line:183][INFO] eval_ppl = 3.847275759705999
[2021-06-23 12:59:19,043][train.py][line:183][INFO] eval_acc = 0.9941884507232082
[2021-06-23 13:02:27,293][train.py][line:181][INFO] #####Evaluate: global step 2600#####
[2021-06-23 13:02:27,294][train.py][line:183][INFO] eval_loss_per_word = 1.2900992694258826
[2021-06-23 13:02:27,294][train.py][line:183][INFO] eval_ppl = 3.633147198288645
[2021-06-23 13:02:27,294][train.py][line:183][INFO] eval_acc = 0.9969345589992097
[2021-06-23 13:05:35,848][train.py][line:181][INFO] #####Evaluate: global step 2800#####
[2021-06-23 13:05:35,849][train.py][line:183][INFO] eval_loss_per_word = 1.2985417113474826
[2021-06-23 13:05:35,850][train.py][line:183][INFO] eval_ppl = 3.663949673604063
[2021-06-23 13:05:35,850][train.py][line:183][INFO] eval_acc = 0.9969207748528159
[2021-06-23 13:08:50,487][train.py][line:181][INFO] #####Evaluate: global step 3000#####
[2021-06-23 13:08:50,488][train.py][line:183][INFO] eval_loss_per_word = 1.4854154271005795
[2021-06-23 13:08:50,488][train.py][line:183][INFO] eval_ppl = 4.416799890078577
[2021-06-23 13:08:50,488][train.py][line:183][INFO] eval_acc = 0.9950223915800308
[2021-06-23 13:08:50,863][train.py][line:191][INFO] global step 3000: save model to ./checkpoints/3000.pth
[2021-06-23 13:11:59,477][train.py][line:181][INFO] #####Evaluate: global step 3200#####
[2021-06-23 13:11:59,478][train.py][line:183][INFO] eval_loss_per_word = 1.3226386607052947
[2021-06-23 13:11:59,479][train.py][line:183][INFO] eval_ppl = 3.753312039464585
[2021-06-23 13:11:59,479][train.py][line:183][INFO] eval_acc = 0.996039355269526
[2021-06-23 13:15:07,675][train.py][line:181][INFO] #####Evaluate: global step 3400#####
[2021-06-23 13:15:07,676][train.py][line:183][INFO] eval_loss_per_word = 1.314427822655757
[2021-06-23 13:15:07,677][train.py][line:183][INFO] eval_ppl = 3.7226203767256005
[2021-06-23 13:15:07,678][train.py][line:183][INFO] eval_acc = 0.9947934215927122
[2021-06-23 13:18:15,653][train.py][line:181][INFO] #####Evaluate: global step 3600#####
[2021-06-23 13:18:15,656][train.py][line:183][INFO] eval_loss_per_word = 1.302927534774099
[2021-06-23 13:18:15,656][train.py][line:183][INFO] eval_ppl = 3.680054400345283
[2021-06-23 13:18:15,656][train.py][line:183][INFO] eval_acc = 0.9968579804081332
[2021-06-23 13:21:23,997][train.py][line:181][INFO] #####Evaluate: global step 3800#####
[2021-06-23 13:21:23,999][train.py][line:183][INFO] eval_loss_per_word = 1.3104329591109092
[2021-06-23 13:21:23,999][train.py][line:183][INFO] eval_ppl = 3.7077786813034366
[2021-06-23 13:21:24,000][train.py][line:183][INFO] eval_acc = 0.996637434065833
[2021-06-23 13:24:32,098][train.py][line:181][INFO] #####Evaluate: global step 4000#####
[2021-06-23 13:24:32,099][train.py][line:183][INFO] eval_loss_per_word = 1.3285749784933814
[2021-06-23 13:24:32,100][train.py][line:183][INFO] eval_ppl = 3.7756591566576
[2021-06-23 13:24:32,100][train.py][line:183][INFO] eval_acc = 0.9945407122421599
[2021-06-23 13:24:32,485][train.py][line:191][INFO] global step 4000: save model to ./checkpoints/4000.pth
[2021-06-23 13:27:40,219][train.py][line:181][INFO] #####Evaluate: global step 4200#####
[2021-06-23 13:27:40,221][train.py][line:183][INFO] eval_loss_per_word = 1.482370326540206
[2021-06-23 13:27:40,221][train.py][line:183][INFO] eval_ppl = 4.403370747180486
[2021-06-23 13:27:40,221][train.py][line:183][INFO] eval_acc = 0.9818700185626504
[2021-06-23 13:30:54,084][train.py][line:181][INFO] #####Evaluate: global step 4400#####
[2021-06-23 13:30:54,086][train.py][line:183][INFO] eval_loss_per_word = 1.4303912550084792
[2021-06-23 13:30:54,086][train.py][line:183][INFO] eval_ppl = 4.180334448791373
[2021-06-23 13:30:54,086][train.py][line:183][INFO] eval_acc = 0.9960875997819042
[2021-06-23 13:34:02,112][train.py][line:181][INFO] #####Evaluate: global step 4600#####
[2021-06-23 13:34:02,113][train.py][line:183][INFO] eval_loss_per_word = 1.3019637042127032
[2021-06-23 13:34:02,114][train.py][line:183][INFO] eval_ppl = 3.6765091602264697
[2021-06-23 13:34:02,114][train.py][line:183][INFO] eval_acc = 0.9969276669260129
[2021-06-23 13:37:10,258][train.py][line:181][INFO] #####Evaluate: global step 4800#####
[2021-06-23 13:37:10,259][train.py][line:183][INFO] eval_loss_per_word = 1.2914353168101782
[2021-06-23 13:37:10,259][train.py][line:183][INFO] eval_ppl = 3.6380044991692153
[2021-06-23 13:37:10,260][train.py][line:183][INFO] eval_acc = 0.9972117734989064
[2021-06-23 13:40:18,052][train.py][line:181][INFO] #####Evaluate: global step 5000#####
[2021-06-23 13:40:18,053][train.py][line:183][INFO] eval_loss_per_word = 1.4504528080124945
[2021-06-23 13:40:18,054][train.py][line:183][INFO] eval_ppl = 4.265045324689488
[2021-06-23 13:40:18,054][train.py][line:183][INFO] eval_acc = 0.9815476226942186
[2021-06-23 13:40:18,442][train.py][line:191][INFO] global step 5000: save model to ./checkpoints/5000.pth
[2021-06-23 13:43:26,017][train.py][line:181][INFO] #####Evaluate: global step 5200#####
[2021-06-23 13:43:26,018][train.py][line:183][INFO] eval_loss_per_word = 1.275664474229581
[2021-06-23 13:43:26,019][train.py][line:183][INFO] eval_ppl = 3.581080155183837
[2021-06-23 13:43:26,019][train.py][line:183][INFO] eval_acc = 0.9979553516182588
[2021-06-23 13:46:33,811][train.py][line:181][INFO] #####Evaluate: global step 5400#####
[2021-06-23 13:46:33,813][train.py][line:183][INFO] eval_loss_per_word = 1.3139740660857926
[2021-06-23 13:46:33,813][train.py][line:183][INFO] eval_ppl = 3.7209315964487266
[2021-06-23 13:46:33,813][train.py][line:183][INFO] eval_acc = 0.9952161354154542
[2021-06-23 13:49:41,396][train.py][line:181][INFO] #####Evaluate: global step 5600#####
[2021-06-23 13:49:41,397][train.py][line:183][INFO] eval_loss_per_word = 1.309725885198258
[2021-06-23 13:49:41,397][train.py][line:183][INFO] eval_ppl = 3.705157934364087
[2021-06-23 13:49:41,398][train.py][line:183][INFO] eval_acc = 0.9959865160416833
[2021-06-23 13:52:54,246][train.py][line:181][INFO] #####Evaluate: global step 5800#####
[2021-06-23 13:52:54,248][train.py][line:183][INFO] eval_loss_per_word = 1.384833471836883
[2021-06-23 13:52:54,248][train.py][line:183][INFO] eval_ppl = 3.9941607091851723
[2021-06-23 13:52:54,248][train.py][line:183][INFO] eval_acc = 0.9976107479584148
[2021-06-23 13:56:01,654][train.py][line:181][INFO] #####Evaluate: global step 6000#####
[2021-06-23 13:56:01,656][train.py][line:183][INFO] eval_loss_per_word = 1.6855504466733495
[2021-06-23 13:56:01,656][train.py][line:183][INFO] eval_ppl = 5.39542000610984
[2021-06-23 13:56:01,656][train.py][line:183][INFO] eval_acc = 0.9288753361800148
[2021-06-23 13:56:02,018][train.py][line:191][INFO] global step 6000: save model to ./checkpoints/6000.pth
[2021-06-23 13:59:09,687][train.py][line:181][INFO] #####Evaluate: global step 6200#####
[2021-06-23 13:59:09,689][train.py][line:183][INFO] eval_loss_per_word = 1.3319513218434018
[2021-06-23 13:59:09,690][train.py][line:183][INFO] eval_ppl = 3.788428623264289
[2021-06-23 13:59:09,690][train.py][line:183][INFO] eval_acc = 0.9942336320919433
[2021-06-23 14:02:17,392][train.py][line:181][INFO] #####Evaluate: global step 6400#####
[2021-06-23 14:02:17,394][train.py][line:183][INFO] eval_loss_per_word = 1.290550949250267
[2021-06-23 14:02:17,394][train.py][line:183][INFO] eval_ppl = 3.6347885882405877
[2021-06-23 14:02:17,395][train.py][line:183][INFO] eval_acc = 0.9929203092549822
[2021-06-23 14:05:24,906][train.py][line:181][INFO] #####Evaluate: global step 6600#####
[2021-06-23 14:05:24,907][train.py][line:183][INFO] eval_loss_per_word = 1.2606302242154142
[2021-06-23 14:05:24,908][train.py][line:183][INFO] eval_ppl = 3.52764399362133
[2021-06-23 14:05:24,908][train.py][line:183][INFO] eval_acc = 0.9973029020222874
[2021-06-23 14:08:46,977][train.py][line:181][INFO] #####Evaluate: global step 6800#####
[2021-06-23 14:08:46,979][train.py][line:183][INFO] eval_loss_per_word = 1.2591731026385728
[2021-06-23 14:08:46,980][train.py][line:183][INFO] eval_ppl = 3.522507530577188
[2021-06-23 14:08:46,980][train.py][line:183][INFO] eval_acc = 0.9981858531773989
[2021-06-23 14:12:13,001][train.py][line:181][INFO] #####Evaluate: global step 7000#####
[2021-06-23 14:12:13,003][train.py][line:183][INFO] eval_loss_per_word = 1.258286290396882
[2021-06-23 14:12:13,004][train.py][line:183][INFO] eval_ppl = 3.519385112481551
[2021-06-23 14:12:13,004][train.py][line:183][INFO] eval_acc = 0.9981406718086638
[2021-06-23 14:12:13,993][train.py][line:191][INFO] global step 7000: save model to ./checkpoints/7000.pth
[2021-06-23 14:15:40,135][train.py][line:181][INFO] #####Evaluate: global step 7200#####
[2021-06-23 14:15:40,136][train.py][line:183][INFO] eval_loss_per_word = 1.3714186369161552
[2021-06-23 14:15:40,137][train.py][line:183][INFO] eval_ppl = 3.940937491125397
[2021-06-23 14:15:40,137][train.py][line:183][INFO] eval_acc = 0.9984753202516679
[2021-06-23 14:19:05,802][train.py][line:181][INFO] #####Evaluate: global step 7400#####
[2021-06-23 14:19:05,805][train.py][line:183][INFO] eval_loss_per_word = 1.2725278233653055
[2021-06-23 14:19:05,806][train.py][line:183][INFO] eval_ppl = 3.569865154985043
[2021-06-23 14:19:05,806][train.py][line:183][INFO] eval_acc = 0.9976926870508666
[2021-06-23 14:22:33,682][train.py][line:181][INFO] #####Evaluate: global step 7600#####
[2021-06-23 14:22:33,685][train.py][line:183][INFO] eval_loss_per_word = 1.2571745188220127
[2021-06-23 14:22:33,687][train.py][line:183][INFO] eval_ppl = 3.515474534390059
[2021-06-23 14:22:33,688][train.py][line:183][INFO] eval_acc = 0.9984891043980616
[2021-06-23 14:26:00,172][train.py][line:181][INFO] #####Evaluate: global step 7800#####
[2021-06-23 14:26:00,174][train.py][line:183][INFO] eval_loss_per_word = 1.2785390774926524
[2021-06-23 14:26:00,174][train.py][line:183][INFO] eval_ppl = 3.591389149919305
[2021-06-23 14:26:00,175][train.py][line:183][INFO] eval_acc = 0.9976023243133963
[2021-06-23 14:29:18,209][train.py][line:181][INFO] #####Evaluate: global step 8000#####
[2021-06-23 14:29:18,212][train.py][line:183][INFO] eval_loss_per_word = 1.2413167887062182
[2021-06-23 14:29:18,213][train.py][line:183][INFO] eval_ppl = 3.4601667747617944
[2021-06-23 14:29:18,214][train.py][line:183][INFO] eval_acc = 0.9987226691008448
[2021-06-23 14:29:19,203][train.py][line:191][INFO] global step 8000: save model to ./checkpoints/8000.pth
[2021-06-23 14:32:45,489][train.py][line:181][INFO] #####Evaluate: global step 8200#####
[2021-06-23 14:32:45,491][train.py][line:183][INFO] eval_loss_per_word = 1.233364283747515
[2021-06-23 14:32:45,492][train.py][line:183][INFO] eval_ppl = 3.432758906376828
[2021-06-23 14:32:45,492][train.py][line:183][INFO] eval_acc = 0.9989815047386832
[2021-06-23 14:36:07,679][train.py][line:181][INFO] #####Evaluate: global step 8400#####
[2021-06-23 14:36:07,684][train.py][line:183][INFO] eval_loss_per_word = 1.2301380456535715
[2021-06-23 14:36:07,686][train.py][line:183][INFO] eval_ppl = 3.4217018547570346
[2021-06-23 14:36:07,687][train.py][line:183][INFO] eval_acc = 0.9990358755383475
