[2021-06-23 11:53:41,884][utils.py][line:129][INFO] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2021-06-23 11:53:41,886][utils.py][line:141][INFO] NumExpr defaulting to 8 threads.
[2021-06-23 11:53:42,659][preprocess.py][line:89][INFO] load src_vocab from ./checkpoints/vocabs/en-src_vocab.pkl
[2021-06-23 11:53:42,786][preprocess.py][line:97][INFO] load tgt_vocab from ./checkpoints/vocabs/zh-tgt_vocab.pkl
[2021-06-23 11:53:42,788][preprocess.py][line:109][INFO] train and val data cache alreadly existed
[2021-06-23 11:53:42,789][preprocess.py][line:111][INFO] src vocab size:24454 tgt vocab size:6870 train examples:282305 val examples:31368
[2021-06-23 11:53:53,846][train.py][line:264][INFO] ==== Model Structure: =====
[2021-06-23 11:53:53,856][train.py][line:265][INFO] Transformer(
  (encoder): Encoder(
    (src_emb): Embedding(24454, 512, padding_idx=1)
    (pos_enc): PosEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (layers): ModuleList(
      (0): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (1): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (2): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (3): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (4): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (5): EncoderLayer(
        (enc_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
    )
  )
  (decoder): Decoder(
    (tgt_emb): Embedding(6870, 512, padding_idx=1)
    (pos_enc): PosEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (layers): ModuleList(
      (0): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (1): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (2): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (3): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (4): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
      (5): DecoderLayer(
        (dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (enc_dec_MHA): MultiHeadAttention(
          (Wq): Linear(in_features=512, out_features=512, bias=True)
          (Wk): Linear(in_features=512, out_features=512, bias=True)
          (Wv): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (project): Linear(in_features=512, out_features=512, bias=True)
          (norm): LayerNorm()
        )
        (FFN): FFN(
          (ffn): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=2048, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (norm): LayerNorm()
        )
      )
    )
  )
  (project): Linear(in_features=512, out_features=6870, bias=False)
)
[2021-06-23 11:53:53,893][train.py][line:267][INFO] ==== Training/Evaluation Parameters: =====
[2021-06-23 11:53:53,894][train.py][line:269][INFO] 	adam_eps=1e-09
[2021-06-23 11:53:53,895][train.py][line:269][INFO] 	batch_size=200
[2021-06-23 11:53:53,896][train.py][line:269][INFO] 	beta1=0.9
[2021-06-23 11:53:53,896][train.py][line:269][INFO] 	beta2=0.98
[2021-06-23 11:53:53,897][train.py][line:269][INFO] 	checkpoints=./checkpoints
[2021-06-23 11:53:53,898][train.py][line:269][INFO] 	cuda=True
[2021-06-23 11:53:53,899][train.py][line:269][INFO] 	d_ff=2048
[2021-06-23 11:53:53,900][train.py][line:269][INFO] 	d_k=64
[2021-06-23 11:53:53,900][train.py][line:269][INFO] 	d_model=512
[2021-06-23 11:53:53,901][train.py][line:269][INFO] 	d_v=64
[2021-06-23 11:53:53,902][train.py][line:269][INFO] 	data_dir=./data/news-commentary-v16.en-zh.tsv
[2021-06-23 11:53:53,903][train.py][line:269][INFO] 	device=cuda
[2021-06-23 11:53:53,903][train.py][line:269][INFO] 	drop=0.1
[2021-06-23 11:53:53,904][train.py][line:269][INFO] 	epochs=10
[2021-06-23 11:53:53,905][train.py][line:269][INFO] 	eval=True
[2021-06-23 11:53:53,906][train.py][line:269][INFO] 	labelsmooth=True
[2021-06-23 11:53:53,907][train.py][line:269][INFO] 	log=True
[2021-06-23 11:53:53,909][train.py][line:269][INFO] 	log_dir=./results/logs
[2021-06-23 11:53:53,910][train.py][line:269][INFO] 	log_step=200
[2021-06-23 11:53:53,911][train.py][line:269][INFO] 	min_freq=1
[2021-06-23 11:53:53,911][train.py][line:269][INFO] 	num_head=8
[2021-06-23 11:53:53,912][train.py][line:269][INFO] 	num_layer=6
[2021-06-23 11:53:53,913][train.py][line:269][INFO] 	overwrite_data_cache=False
[2021-06-23 11:53:53,914][train.py][line:269][INFO] 	overwrite_vocab=False
[2021-06-23 11:53:53,914][train.py][line:269][INFO] 	pad_idx=1
[2021-06-23 11:53:53,915][train.py][line:269][INFO] 	result_dir=./results
[2021-06-23 11:53:53,916][train.py][line:269][INFO] 	save_step=1000
[2021-06-23 11:53:53,916][train.py][line:269][INFO] 	scale_emb=True
[2021-06-23 11:53:53,917][train.py][line:269][INFO] 	seed=123
[2021-06-23 11:53:53,918][train.py][line:269][INFO] 	share_emb_weight=False
[2021-06-23 11:53:53,918][train.py][line:269][INFO] 	share_proj_weight=True
[2021-06-23 11:53:53,918][train.py][line:269][INFO] 	smooth_eps=0.1
[2021-06-23 11:53:53,918][train.py][line:269][INFO] 	src_lang=en
[2021-06-23 11:53:53,918][train.py][line:269][INFO] 	src_max_seq_len=100
[2021-06-23 11:53:53,920][train.py][line:269][INFO] 	src_tokenizer_dir=./models/bert-base-uncased
[2021-06-23 11:53:53,920][train.py][line:269][INFO] 	src_vocab_dir=./checkpoints/vocabs/en-src_vocab.pkl
[2021-06-23 11:53:53,920][train.py][line:269][INFO] 	src_vocab_size=24454
[2021-06-23 11:53:53,920][train.py][line:269][INFO] 	task_name=en_zh_translation
[2021-06-23 11:53:53,920][train.py][line:269][INFO] 	tgt_lang=zh
[2021-06-23 11:53:53,920][train.py][line:269][INFO] 	tgt_max_seq_len=100
[2021-06-23 11:53:53,921][train.py][line:269][INFO] 	tgt_tokenizer_dir=./models/bert-base-chinese
[2021-06-23 11:53:53,921][train.py][line:269][INFO] 	tgt_vocab_dir=./checkpoints/vocabs/zh-tgt_vocab.pkl
[2021-06-23 11:53:53,921][train.py][line:269][INFO] 	tgt_vocab_size=6870
[2021-06-23 11:53:53,921][train.py][line:269][INFO] 	train=True
[2021-06-23 11:53:53,921][train.py][line:269][INFO] 	train_data_cache=./data/train_data_cache.pkl
[2021-06-23 11:53:53,921][train.py][line:269][INFO] 	use_tb=True
[2021-06-23 11:53:53,923][train.py][line:269][INFO] 	val_data_cache=./data/val_data_cache.pkl
[2021-06-23 11:53:53,924][train.py][line:269][INFO] 	vocab_dir=./checkpoints/vocabs
[2021-06-23 11:53:53,924][train.py][line:269][INFO] 	warmup_proportion=0.1
[2021-06-23 11:53:53,924][train.py][line:270][INFO] ==== Parameters End =====

[2021-06-23 11:53:53,924][train.py][line:273][INFO] ==== Train Start=====
[2021-06-23 11:53:54,351][train.py][line:147][INFO] Total train steps: 14120
[2021-06-23 11:53:54,353][train.py][line:148][INFO] ################################
[2021-06-23 11:57:15,859][train.py][line:181][INFO] #####Evaluate: global step 200#####
[2021-06-23 11:57:15,860][train.py][line:183][INFO] eval_loss_per_word = 6.707179133429963
[2021-06-23 11:57:15,861][train.py][line:183][INFO] eval_ppl = 818.2591808563316
[2021-06-23 11:57:15,862][train.py][line:183][INFO] eval_acc = 0.062019469340995274
[2021-06-23 12:00:41,846][train.py][line:181][INFO] #####Evaluate: global step 400#####
[2021-06-23 12:00:41,852][train.py][line:183][INFO] eval_loss_per_word = 6.731887003454843
[2021-06-23 12:00:41,852][train.py][line:183][INFO] eval_ppl = 838.7284571246219
[2021-06-23 12:00:41,854][train.py][line:183][INFO] eval_acc = 0.06229591805478126
[2021-06-23 12:03:52,012][train.py][line:181][INFO] #####Evaluate: global step 600#####
[2021-06-23 12:03:52,014][train.py][line:183][INFO] eval_loss_per_word = 6.644051670590681
[2021-06-23 12:03:52,014][train.py][line:183][INFO] eval_ppl = 768.2011943052413
[2021-06-23 12:03:52,014][train.py][line:183][INFO] eval_acc = 0.06338486561988838
[2021-06-23 12:07:00,552][train.py][line:181][INFO] #####Evaluate: global step 800#####
[2021-06-23 12:07:00,554][train.py][line:183][INFO] eval_loss_per_word = 6.715325979746495
[2021-06-23 12:07:00,554][train.py][line:183][INFO] eval_ppl = 824.9526409241207
[2021-06-23 12:07:00,554][train.py][line:183][INFO] eval_acc = 0.06441867659942045
[2021-06-23 12:10:09,917][train.py][line:181][INFO] #####Evaluate: global step 1000#####
[2021-06-23 12:10:09,918][train.py][line:183][INFO] eval_loss_per_word = 6.717174820260475
[2021-06-23 12:10:09,918][train.py][line:183][INFO] eval_ppl = 826.4792575892878
[2021-06-23 12:10:09,919][train.py][line:183][INFO] eval_acc = 0.06369654048556952
[2021-06-23 12:10:10,300][train.py][line:191][INFO] global step 1000: save model to ./checkpoints/1000.pth
[2021-06-23 12:13:19,681][train.py][line:181][INFO] #####Evaluate: global step 1200#####
[2021-06-23 12:13:19,763][train.py][line:183][INFO] eval_loss_per_word = 6.710720130472689
[2021-06-23 12:13:19,763][train.py][line:183][INFO] eval_ppl = 821.161770193175
[2021-06-23 12:13:19,763][train.py][line:183][INFO] eval_acc = 0.06232118898983649
[2021-06-23 12:16:43,141][train.py][line:181][INFO] #####Evaluate: global step 1400#####
[2021-06-23 12:16:43,144][train.py][line:183][INFO] eval_loss_per_word = 6.799300300815064
[2021-06-23 12:16:43,144][train.py][line:183][INFO] eval_ppl = 897.219288364568
[2021-06-23 12:16:43,145][train.py][line:183][INFO] eval_acc = 0.04220399311405309
